{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "import keras.backend as kbe\n",
    "# Text preprocessing libraries\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "# Model libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATIC VARIABLES\n",
    "POSITIVE_EXAMPLES_FILE_NAME = \"pos_amazon_cell_phone_reviews.json\"\n",
    "NEGATIVE_EXAMPLES_FILE_NAME = \"neg_amazon_cell_phone_reviews.json\"\n",
    "GLOVE_EMBEDDING_FILE_NAME = \"glove.6B/glove.6B.50d.txt\"\n",
    "KINDLE_REVIEWS_FILE_NAME = \"kindle_reviews.json\"\n",
    "HOTEL_REVIEWS_FILE_NAME = \"trip_advisor_1.json\"\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def precision(y_true, y_pred): # True positive rate\n",
    "    true_positives = kbe.sum(kbe.round(kbe.clip(y_true * y_pred, 0, 1))) # get true_positives from true vs predicted\n",
    "    predicted_positives = kbe.sum(kbe.round(kbe.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives/kbe.sum(predicted_positives + kbe.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred): # False negative rate\n",
    "    true_positives = kbe.sum(kbe.round(kbe.clip(y_true * y_pred, 0, 1))) \n",
    "    predicted_positives = kbe.sum(kbe.round(kbe.clip(y_true, 0, 1)))\n",
    "    recall = true_positives/kbe.sum(predicted_positives + kbe.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive data loaded:  108664 entries\n",
      "Negative data loaded:  13279 entries\n",
      "Number of kindle review entries:  982619  entries\n",
      "Numer of hotel review entries:  2920  entries\n"
     ]
    }
   ],
   "source": [
    "# Reading data from file\n",
    "positive_data = json.loads(open(POSITIVE_EXAMPLES_FILE_NAME, \"r\").read())['root']\n",
    "negative_data = json.loads(open(NEGATIVE_EXAMPLES_FILE_NAME, \"r\").read())['root']\n",
    "print(\"Positive data loaded: \", len(positive_data), \"entries\")\n",
    "print(\"Negative data loaded: \", len(negative_data), \"entries\")\n",
    "\n",
    "# Transfer Learning file read\n",
    "kindle_review_data = []\n",
    "with open(KINDLE_REVIEWS_FILE_NAME, \"r\") as f:\n",
    "    kindle_review_data = [json.loads(line) for line in f]\n",
    "print(\"Number of kindle review entries: \", len(kindle_review_data), \" entries\")\n",
    "\n",
    "# Second transfer learning - different domain\n",
    "hotel_review_data = json.loads(open(HOTEL_REVIEWS_FILE_NAME, \"r\").read())[\"Reviews\"]\n",
    "print(\"Numer of hotel review entries: \", len(hotel_review_data), \" entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We stayed here because of its location near Xcaret and Xplor. There are some negatives and posititves about it. It is owned by the same company that owns Xcaret, so we got a discount going there. The grounds and pools are beautiful, but we were acosted several times in the lobby by agents wanting us to buy into the time share \"opportunity\". Late at night, drunk party goers sang and laughed in the halls and the echos were rather noisy. But we always carry earplugs with us, so that problem was solved easily. We don't drink,and are vegetarians,so I would rather have the money you pay for having alcohol included, to have better quality food options. Everything was oily and fried. We stayed in building 12, which was close to the lobby and fitness center. It was okay,but our room looked out over other rooms with a little canal between, not a great view. The beach here is mostly rock. Only a small bay was dredged to give guests a beach.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Process reviews into examples\n",
    "positive_examples = []\n",
    "negative_examples = []\n",
    "\n",
    "for example in positive_data:\n",
    "    positive_examples.append(example[\"summary\"] + \" \" + example[\"text\"])\n",
    "for example in negative_data:\n",
    "    negative_examples.append(example[\"summary\"] + \" \" + example[\"text\"])\n",
    "\n",
    "training_examples = positive_examples + negative_examples\n",
    "training_labels = [1]*len(positive_examples) + [0]*len(negative_examples)\n",
    "training_labels = numpy.array(training_labels)\n",
    "\n",
    "# Transfer Learning Examples\n",
    "transfer_training_examples = []\n",
    "transfer_training_labels = []\n",
    "\n",
    "for data in kindle_review_data:\n",
    "    transfer_training_examples.append(data[\"summary\"] + \" \" + data[\"reviewText\"])\n",
    "    label = 1 if data[\"overall\"] > 2.5 else 0\n",
    "    transfer_training_labels.append(label)\n",
    "transfer_training_labels = numpy.array(transfer_training_labels)\n",
    "\n",
    "# Hotel Review Transfer Learning Examples\n",
    "hotel_examples = []\n",
    "hotel_labels = []\n",
    "\n",
    "for example in hotel_review_data:\n",
    "    hotel_examples.append(example[\"Content\"])\n",
    "    label = 1 if float(example[\"Ratings\"][\"Overall\"]) > 2.5 else 0\n",
    "    hotel_labels.append(label)\n",
    "\n",
    "hotel_labels = numpy.array(hotel_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374942\n"
     ]
    }
   ],
   "source": [
    "# Process each examples into sequences to be fed into the LSTM network\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_examples + transfer_training_examples + hotel_examples) # Map each word to a numerical index\n",
    "word_indices = tokenizer.word_index # Get the word to index map\n",
    "training_sequences = tokenizer.texts_to_sequences(training_examples)# Replace each word in the examples with it's equivalent numerical index\n",
    "transfer_training_sequences = tokenizer.texts_to_sequences(transfer_training_examples)\n",
    "hotel_sequences = tokenizer.texts_to_sequences(hotel_examples)\n",
    "training_sequences_padded = sequence.pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH) # Pad examples that are too short with 0s\n",
    "transfer_training_sequences_padded = sequence.pad_sequences(transfer_training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "hotel_sequences_padded = sequence.pad_sequences(hotel_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"Number of unique words: \", len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split data into training, validation and test sets\n",
    "random_indices = numpy.arange(training_labels.shape[0])\n",
    "numpy.random.shuffle(random_indices) # Shuffle the indices randomly\n",
    "training_sequences_padded = training_sequences_padded[random_indices] # Do array indexing by the random shuffled indices\n",
    "training_labels = training_labels[random_indices]\n",
    "random_indices = numpy.arange(transfer_training_labels.shape[0])\n",
    "numpy.random.shuffle(random_indices)\n",
    "transfer_training_sequences_padded = transfer_training_sequences_padded[random_indices]\n",
    "transfer_training_labels = transfer_training_labels[random_indices]\n",
    "\n",
    "validation_size = int((VALIDATION_SPLIT + TEST_SPLIT) * training_labels.shape[0])\n",
    "test_size = int(TEST_SPLIT * training_labels.shape[0])\n",
    "\n",
    "x_train = training_sequences_padded[:-validation_size]\n",
    "y_train = training_labels[:-validation_size]\n",
    "x_val = training_sequences_padded[-validation_size:-test_size]\n",
    "y_val = training_labels[-validation_size:-test_size]\n",
    "x_test = training_sequences_padded[-test_size:]\n",
    "y_test = training_labels[-test_size:]\n",
    "\n",
    "# Transfer Learning Set\n",
    "x_transfer_train = training_sequences_padded[:-validation_size]\n",
    "y_transfer_train = training_labels[:-validation_size]\n",
    "x_transfer_val = training_sequences_padded[-validation_size:-test_size]\n",
    "y_transfer_val = training_labels[-validation_size:-test_size]\n",
    "x_transfer_test = training_sequences_padded[-test_size:]\n",
    "y_transfer_test = training_labels[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe embedding to map similarities into an embedding matrix\n",
    "glove_embeddings = {} # dictionary of word to it's respective embedding list\n",
    "with open(GLOVE_EMBEDDING_FILE_NAME, \"r\", encoding='utf-8') as glove_file:\n",
    "    for line in glove_file:\n",
    "        embedding_list = line.split()\n",
    "        word = embedding_list[0]\n",
    "        glove_embeddings[word] = numpy.asarray(embedding_list[1:], dtype='float32')\n",
    "\n",
    "embedding_matrix = numpy.zeros((len(word_indices) + 1, MAX_SEQUENCE_LENGTH)) # initialize matrix of embeddings for each index in word_indices. Words that are not present in the embedding are initialized to 0\n",
    "for word, index in word_indices.items():\n",
    "    embedding_list = glove_embeddings.get(word)\n",
    "    if embedding_list is not None:\n",
    "        embedding_matrix[index] = embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "embedding_layer = Embedding(len(word_indices) + 1, \n",
    "                            MAX_SEQUENCE_LENGTH, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "                            trainable=False)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(embedding_layer)\n",
    "model_lstm.add(LSTM(32))\n",
    "model_lstm.add(Dropout(0.4)) #Dropout layer for regularization\n",
    "model_lstm.add(Dense(1, activation='sigmoid')) # output layer using sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85361 samples, validate on 24388 samples\n",
      "Epoch 1/20\n",
      "85361/85361 [==============================] - 29s 342us/step - loss: 0.2775 - acc: 0.9014 - precision: 0.9102 - recall: 0.9873 - val_loss: 0.1992 - val_acc: 0.9226 - val_precision: 0.9351 - val_recall: 0.9810\n",
      "Epoch 2/20\n",
      "85361/85361 [==============================] - 28s 323us/step - loss: 0.1945 - acc: 0.9265 - precision: 0.9402 - recall: 0.9800 - val_loss: 0.1745 - val_acc: 0.9328 - val_precision: 0.9539 - val_recall: 0.9714\n",
      "Epoch 3/20\n",
      "85361/85361 [==============================] - 28s 330us/step - loss: 0.1723 - acc: 0.9351 - precision: 0.9475 - recall: 0.9816 - val_loss: 0.1565 - val_acc: 0.9401 - val_precision: 0.9549 - val_recall: 0.9788\n",
      "Epoch 4/20\n",
      "85361/85361 [==============================] - 28s 331us/step - loss: 0.1585 - acc: 0.9406 - precision: 0.9528 - recall: 0.9821 - val_loss: 0.1510 - val_acc: 0.9430 - val_precision: 0.9513 - val_recall: 0.9864\n",
      "Epoch 5/20\n",
      "85361/85361 [==============================] - 28s 327us/step - loss: 0.1490 - acc: 0.9442 - precision: 0.9560 - recall: 0.9828 - val_loss: 0.1420 - val_acc: 0.9463 - val_precision: 0.9562 - val_recall: 0.9847\n",
      "Epoch 6/20\n",
      "85361/85361 [==============================] - 28s 327us/step - loss: 0.1413 - acc: 0.9482 - precision: 0.9588 - recall: 0.9841 - val_loss: 0.1380 - val_acc: 0.9478 - val_precision: 0.9541 - val_recall: 0.9888\n",
      "Epoch 7/20\n",
      "85361/85361 [==============================] - 28s 327us/step - loss: 0.1374 - acc: 0.9494 - precision: 0.9600 - recall: 0.9843 - val_loss: 0.1335 - val_acc: 0.9496 - val_precision: 0.9648 - val_recall: 0.9789\n",
      "Epoch 8/20\n",
      "85361/85361 [==============================] - 28s 325us/step - loss: 0.1323 - acc: 0.9515 - precision: 0.9618 - recall: 0.9848 - val_loss: 0.1298 - val_acc: 0.9519 - val_precision: 0.9652 - val_recall: 0.9812\n",
      "Epoch 9/20\n",
      "85361/85361 [==============================] - 28s 325us/step - loss: 0.1276 - acc: 0.9533 - precision: 0.9635 - recall: 0.9850 - val_loss: 0.1274 - val_acc: 0.9532 - val_precision: 0.9642 - val_recall: 0.9837\n",
      "Epoch 10/20\n",
      "85361/85361 [==============================] - 29s 342us/step - loss: 0.1239 - acc: 0.9550 - precision: 0.9646 - recall: 0.9858 - val_loss: 0.1304 - val_acc: 0.9513 - val_precision: 0.9718 - val_recall: 0.9734\n",
      "Epoch 11/20\n",
      "85361/85361 [==============================] - 27s 316us/step - loss: 0.1208 - acc: 0.9566 - precision: 0.9663 - recall: 0.9859 - val_loss: 0.1257 - val_acc: 0.9538 - val_precision: 0.9674 - val_recall: 0.9811\n",
      "Epoch 12/20\n",
      "85361/85361 [==============================] - 636s 7ms/step - loss: 0.1165 - acc: 0.9581 - precision: 0.9671 - recall: 0.9866 - val_loss: 0.1238 - val_acc: 0.9546 - val_precision: 0.9659 - val_recall: 0.9836\n",
      "Epoch 13/20\n",
      "85361/85361 [==============================] - 28s 324us/step - loss: 0.1133 - acc: 0.9592 - precision: 0.9679 - recall: 0.9870 - val_loss: 0.1233 - val_acc: 0.9553 - val_precision: 0.9644 - val_recall: 0.9860\n",
      "Epoch 14/20\n",
      "85361/85361 [==============================] - 27s 322us/step - loss: 0.1110 - acc: 0.9597 - precision: 0.9686 - recall: 0.9868 - val_loss: 0.1203 - val_acc: 0.9547 - val_precision: 0.9675 - val_recall: 0.9820\n",
      "Epoch 15/20\n",
      "85361/85361 [==============================] - 28s 325us/step - loss: 0.1082 - acc: 0.9612 - precision: 0.9694 - recall: 0.9876 - val_loss: 0.1206 - val_acc: 0.9565 - val_precision: 0.9680 - val_recall: 0.9835\n",
      "Epoch 16/20\n",
      "85361/85361 [==============================] - 28s 323us/step - loss: 0.1067 - acc: 0.9618 - precision: 0.9703 - recall: 0.9874 - val_loss: 0.1272 - val_acc: 0.9545 - val_precision: 0.9714 - val_recall: 0.9776\n",
      "Epoch 17/20\n",
      "85361/85361 [==============================] - 28s 329us/step - loss: 0.1045 - acc: 0.9619 - precision: 0.9704 - recall: 0.9875 - val_loss: 0.1265 - val_acc: 0.9559 - val_precision: 0.9630 - val_recall: 0.9883\n",
      "Epoch 18/20\n",
      "85361/85361 [==============================] - 30s 346us/step - loss: 0.1022 - acc: 0.9635 - precision: 0.9716 - recall: 0.9879 - val_loss: 0.1328 - val_acc: 0.9555 - val_precision: 0.9593 - val_recall: 0.9919\n",
      "Epoch 19/20\n",
      "85361/85361 [==============================] - 35s 404us/step - loss: 0.1004 - acc: 0.9643 - precision: 0.9724 - recall: 0.9880 - val_loss: 0.1228 - val_acc: 0.9559 - val_precision: 0.9729 - val_recall: 0.9777\n",
      "Epoch 20/20\n",
      "85361/85361 [==============================] - 30s 352us/step - loss: 0.0984 - acc: 0.9644 - precision: 0.9726 - recall: 0.9879 - val_loss: 0.1220 - val_acc: 0.9559 - val_precision: 0.9622 - val_recall: 0.9892\n",
      "12194/12194 [==============================] - 3s 229us/step\n",
      "Loss:  0.1274158432074284\n",
      "Accuracy:  0.9543217976053797\n",
      "Precision:  0.9604440935912906\n",
      "Recall:  0.9896093709206452\n"
     ]
    }
   ],
   "source": [
    "# Train model and output training results\n",
    "model_lstm.compile('adam', 'binary_crossentropy', metrics=['accuracy', precision, recall])\n",
    "model_lstm.fit(x_train, y_train,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         epochs=20,\n",
    "         validation_data=[x_val, y_val])\n",
    "results = model_lstm.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Loss: \", results[0])\n",
    "print(\"Accuracy: \", results[1])\n",
    "print(\"Precision: \", results[2])\n",
    "print(\"Recall: \", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12194/12194 [==============================] - 3s 229us/step\n",
      "Loss:  0.1274158432074284\n",
      "Accuracy:  0.9543217976053797\n",
      "Precision:  0.9604440935912906\n",
      "Recall:  0.9896093709206452\n"
     ]
    }
   ],
   "source": [
    "# Test kindle review test set\n",
    "results = model_lstm.evaluate(x_transfer_test, y_transfer_test)\n",
    "\n",
    "print(\"Loss: \", results[0])\n",
    "print(\"Accuracy: \", results[1])\n",
    "print(\"Precision: \", results[2])\n",
    "print(\"Recall: \", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2920/2920 [==============================] - 1s 239us/step\n",
      "Loss:  0.4860623988386703\n",
      "Accuracy:  0.8496575342465753\n",
      "Precision:  0.862609209099861\n",
      "Recall:  0.9738314458768662\n"
     ]
    }
   ],
   "source": [
    "# Test hotel review test set\n",
    "results = model_lstm.evaluate(hotel_sequences_padded, hotel_labels)\n",
    "\n",
    "print(\"Loss: \", results[0])\n",
    "print(\"Accuracy: \", results[1])\n",
    "print(\"Precision: \", results[2])\n",
    "print(\"Recall: \", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
