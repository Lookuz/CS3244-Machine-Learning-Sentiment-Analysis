{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "import keras.backend as kbe\n",
    "# Text preprocessing libraries\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "# Model libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STATIC VARIABLES\n",
    "POSITIVE_EXAMPLES_FILE_NAME = \"pos_amazon_cell_phone_reviews.json\"\n",
    "NEGATIVE_EXAMPLES_FILE_NAME = \"neg_amazon_cell_phone_reviews.json\"\n",
    "GLOVE_EMBEDDING_FILE_NAME = \"glove.6B/glove.6B.50d.txt\"\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "TEST_SPLIT = 0.1\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def precision(y_true, y_pred): # True positive rate\n",
    "    true_positives = kbe.sum(kbe.round(kbe.clip(y_true * y_pred, 0, 1))) # get true_positives from true vs predicted\n",
    "    predicted_positives = kbe.sum(kbe.round(kbe.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives/kbe.sum(predicted_positives + kbe.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred): # False negative rate\n",
    "    true_positives = kbe.sum(kbe.round(kbe.clip(y_true * y_pred, 0, 1))) \n",
    "    predicted_positives = kbe.sum(kbe.round(kbe.clip(y_true, 0, 1)))\n",
    "    recall = true_positives/kbe.sum(predicted_positives + kbe.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive data loaded:  108664 entries\n",
      "Negative data loaded:  13279 entries\n"
     ]
    }
   ],
   "source": [
    "# Reading data from file\n",
    "positive_data = json.loads(open(POSITIVE_EXAMPLES_FILE_NAME, \"r\").read())['root']\n",
    "negative_data = json.loads(open(NEGATIVE_EXAMPLES_FILE_NAME, \"r\").read())['root']\n",
    "print(\"Positive data loaded: \", len(positive_data), \"entries\")\n",
    "print(\"Negative data loaded: \", len(negative_data), \"entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process reviews into examples\n",
    "positive_examples = []\n",
    "negative_examples = []\n",
    "\n",
    "for example in positive_data:\n",
    "    positive_examples.append(example[\"summary\"] + \" \" + example[\"text\"])\n",
    "for example in negative_data:\n",
    "    negative_examples.append(example[\"summary\"] + \" \" + example[\"text\"])\n",
    "\n",
    "training_examples = positive_examples + negative_examples\n",
    "training_labels = [1]*len(positive_examples) + [0]*len(negative_examples)\n",
    "training_labels = numpy.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each examples into sequences to be fed into the LSTM network\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(training_examples) # Map each word to a numerical index\n",
    "word_indices = tokenizer.word_index # Get the word to index map\n",
    "training_sequences = tokenizer.texts_to_sequences(training_examples) # Replace each word in the examples with it's equivalent numerical index\n",
    "training_sequences_padded = sequence.pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH) # Pad examples that are too short with 0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split data into training, validation and test sets\n",
    "random_indices = numpy.arange(training_labels.shape[0])\n",
    "numpy.random.shuffle(random_indices) # Shuffle the indices randomly\n",
    "training_sequences_padded = training_sequences_padded[random_indices] # Do array indexing by the random shuffled indices\n",
    "training_labels = training_labels[random_indices]\n",
    "\n",
    "validation_size = int((VALIDATION_SPLIT + TEST_SPLIT) * training_labels.shape[0])\n",
    "test_size = int(TEST_SPLIT * training_labels.shape[0])\n",
    "\n",
    "x_train = training_sequences_padded[:-validation_size]\n",
    "y_train = training_labels[:-validation_size]\n",
    "x_val = training_sequences_padded[-validation_size:-test_size]\n",
    "y_val = training_labels[-validation_size:-test_size]\n",
    "x_test = training_sequences_padded[-test_size:]\n",
    "y_test = training_labels[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe embedding to map similarities into an embedding matrix\n",
    "glove_embeddings = {} # dictionary of word to it's respective embedding list\n",
    "with open(GLOVE_EMBEDDING_FILE_NAME, \"r\") as glove_file:\n",
    "    for line in glove_file:\n",
    "        embedding_list = line.split()\n",
    "        word = embedding_list[0]\n",
    "        glove_embeddings[word] = numpy.asarray(embedding_list[1:], dtype='float32')\n",
    "\n",
    "embedding_matrix = numpy.zeros((len(word_indices) + 1, MAX_SEQUENCE_LENGTH)) # initialize matrix of embeddings for each index in word_indices. Words that are not present in the embedding are initialized to 0\n",
    "for word, index in word_indices.items():\n",
    "    embedding_list = glove_embeddings.get(word)\n",
    "    if embedding_list is not None:\n",
    "        embedding_matrix[index] = embedding_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "embedding_layer = Embedding(len(word_indices) + 1, \n",
    "                            MAX_SEQUENCE_LENGTH, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=MAX_SEQUENCE_LENGTH, \n",
    "                            trainable=False)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(embedding_layer)\n",
    "model_lstm.add(LSTM(64))\n",
    "model_lstm.add(Dropout(0.1)) #Dropout layer for regularization\n",
    "model_lstm.add(Dense(1, activation='sigmoid')) # output layer using sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85361 samples, validate on 24388 samples\n",
      "Epoch 1/16\n",
      "85361/85361 [==============================] - 30s 353us/step - loss: 0.2493 - acc: 0.9061 - precision: 0.9166 - recall: 0.9846 - val_loss: 0.1882 - val_acc: 0.9269 - val_precision: 0.9366 - val_recall: 0.9845\n",
      "Epoch 2/16\n",
      "85361/85361 [==============================] - 31s 359us/step - loss: 0.1771 - acc: 0.9319 - precision: 0.9439 - recall: 0.9821 - val_loss: 0.1843 - val_acc: 0.9287 - val_precision: 0.9663 - val_recall: 0.9532\n",
      "Epoch 3/16\n",
      "85361/85361 [==============================] - 33s 382us/step - loss: 0.1544 - acc: 0.9414 - precision: 0.9531 - recall: 0.9828 - val_loss: 0.1491 - val_acc: 0.9447 - val_precision: 0.9497 - val_recall: 0.9904\n",
      "Epoch 4/16\n",
      "85361/85361 [==============================] - 33s 384us/step - loss: 0.1387 - acc: 0.9482 - precision: 0.9590 - recall: 0.9841 - val_loss: 0.1488 - val_acc: 0.9455 - val_precision: 0.9497 - val_recall: 0.9913\n",
      "Epoch 5/16\n",
      "85361/85361 [==============================] - 31s 358us/step - loss: 0.1292 - acc: 0.9529 - precision: 0.9623 - recall: 0.9858 - val_loss: 0.1370 - val_acc: 0.9496 - val_precision: 0.9575 - val_recall: 0.9872\n",
      "Epoch 6/16\n",
      "85361/85361 [==============================] - 30s 353us/step - loss: 0.1205 - acc: 0.9553 - precision: 0.9650 - recall: 0.9857 - val_loss: 0.1351 - val_acc: 0.9503 - val_precision: 0.9687 - val_recall: 0.9757\n",
      "Epoch 7/16\n",
      "85361/85361 [==============================] - 30s 356us/step - loss: 0.1132 - acc: 0.9585 - precision: 0.9674 - recall: 0.9868 - val_loss: 0.1270 - val_acc: 0.9544 - val_precision: 0.9679 - val_recall: 0.9813\n",
      "Epoch 8/16\n",
      "85361/85361 [==============================] - 39s 462us/step - loss: 0.1069 - acc: 0.9610 - precision: 0.9696 - recall: 0.9872 - val_loss: 0.1281 - val_acc: 0.9538 - val_precision: 0.9726 - val_recall: 0.9756\n",
      "Epoch 9/16\n",
      "85361/85361 [==============================] - 37s 431us/step - loss: 0.1007 - acc: 0.9634 - precision: 0.9714 - recall: 0.9880 - val_loss: 0.1220 - val_acc: 0.9566 - val_precision: 0.9671 - val_recall: 0.9847\n",
      "Epoch 10/16\n",
      "85361/85361 [==============================] - 32s 375us/step - loss: 0.0966 - acc: 0.9648 - precision: 0.9726 - recall: 0.9884 - val_loss: 0.1223 - val_acc: 0.9572 - val_precision: 0.9648 - val_recall: 0.9879\n",
      "Epoch 11/16\n",
      "85361/85361 [==============================] - 38s 450us/step - loss: 0.0904 - acc: 0.9673 - precision: 0.9748 - recall: 0.9889 - val_loss: 0.1303 - val_acc: 0.9558 - val_precision: 0.9600 - val_recall: 0.9917\n",
      "Epoch 12/16\n",
      "85361/85361 [==============================] - 31s 360us/step - loss: 0.0850 - acc: 0.9696 - precision: 0.9764 - recall: 0.9899 - val_loss: 0.1242 - val_acc: 0.9587 - val_precision: 0.9683 - val_recall: 0.9858\n",
      "Epoch 13/16\n",
      "85361/85361 [==============================] - 30s 352us/step - loss: 0.0816 - acc: 0.9704 - precision: 0.9772 - recall: 0.9899 - val_loss: 0.1250 - val_acc: 0.9568 - val_precision: 0.9687 - val_recall: 0.9833\n",
      "Epoch 14/16\n",
      "85361/85361 [==============================] - 31s 364us/step - loss: 0.0759 - acc: 0.9727 - precision: 0.9790 - recall: 0.9906 - val_loss: 0.1241 - val_acc: 0.9574 - val_precision: 0.9687 - val_recall: 0.9839\n",
      "Epoch 15/16\n",
      "85361/85361 [==============================] - 30s 351us/step - loss: 0.0708 - acc: 0.9748 - precision: 0.9807 - recall: 0.9913 - val_loss: 0.1260 - val_acc: 0.9563 - val_precision: 0.9676 - val_recall: 0.9839\n",
      "Epoch 16/16\n",
      "85361/85361 [==============================] - 32s 376us/step - loss: 0.0674 - acc: 0.9761 - precision: 0.9819 - recall: 0.9915 - val_loss: 0.1308 - val_acc: 0.9581 - val_precision: 0.9738 - val_recall: 0.9793\n",
      "12194/12194 [==============================] - 3s 226us/step\n",
      "Loss:  0.1327534392131026\n",
      "Accuracy:  0.9562899786780383\n",
      "Precision:  0.9756934450789907\n",
      "Recall:  0.9751856860047738\n"
     ]
    }
   ],
   "source": [
    "# Train model and output training results\n",
    "model_lstm.compile('adam', 'binary_crossentropy', metrics=['accuracy', precision, recall])\n",
    "model_lstm.fit(x_train, y_train,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         epochs=16,\n",
    "         validation_data=[x_val, y_val])\n",
    "results = model_lstm.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Loss: \", results[0])\n",
    "print(\"Accuracy: \", results[1])\n",
    "print(\"Precision: \", results[2])\n",
    "print(\"Recall: \", results[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
